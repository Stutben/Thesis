\chapter{Algorithmus mit Warteschlangen-Theorie}

Lohrmann et al. beschreiben in ihrem Paper \cite{lohrmann_elastic_2015} einen Algorithmus, der das Ziel verfolgt, die Latenz der Tupel einer Topologie unter einem, durch den Benutzer bestimmten, Maximalwert zu halten.
Dieses Ziel soll mit einem möglichst geringen Verbrauch von Ressourcen erreicht werden.
Die Latenz eines Tupels bestimmt sich aus der Zeit, welche das Tupel benötigt um von der Quelle zum Konsument zu gelangen.

Dementsprechend ist die Wahl des Pfades für die Latenz des Tupels essentiell, da die sie mindestens die Summe der Latenz aller Operatoren in einem Pfad ist.
Außerdem fließt die Zeit, in der Tupel sich zwischen Operatoren bewegen, in die Latenz des Tupels mit ein.
Einerseits beinhaltet dies die Latenz des Netzwerks, über das die Tupel versendet werden.
Diese Latenz kann jedoch nicht durch reines Skalieren der Operatoren beeinflusst werden, sondern ist von deren Platzierung abhängig.
Im Modell des Algorithmus wird die Netzwerklatenz nicht explizit berücksichtigt.
Der zweite Faktor ist die Zeit, welche zwischen der Ankunft eines Tupels im Zwischenspeicher des Operators und der Bearbeitung des Tupels durch den Operator liegt.
Diese Zeit zwischen Ankunft und Bearbeitung wird in dem Algorithmus durch ein Modell aus der Warteschlangentheorie abgebildet.
Die Dauer, in der sich ein Tupel in der Warteschlange vor der Bearbeitung durch den Operator befindet, wird folgend als Wartezeit bezeichnet.
Der Algorithmus bedient sich der Kingman-Formel aus der Warteschlangen-Theorie um die Wartezeit zu berechnen.
Sie modelliert eine Warteschlange mit einem einzelnen Abnehmer.
Da die genannten Faktoren alle vom gewählten Pfad des Tupels abhängig sind, kann man auch von der Latenz eines Pfades sprechen.

Der Algorithmus von Lohrmann et al. berechnet mit Hilfe der Warteschlangentheorie die Latenz eines Pfades.
Diese Berechnung wird für alle Pfade der Topologie ausgeführt.
Er vergleicht pro Pfad die berechneten Werte mit dem für den Pfad gegebenen Maximalwert für die Latenz.
Der Maximalwert der Latenz des Pfades muss vom Benutzer angegeben werden, bevor der Algorithmus ausgeführt wird.
Der Algorithmus versucht den gegebenen Maximalwert für die Latenz des Pfades mit dem geringsten Ressourcenverbrauch zu erreichen.
Die Latenz eines Operators wird im Modell des Algorithmus als konstant angenommen. 
Die Wartezeit verändert sich nach der Formel von Kingman mit der Parallelisierungsgrad des Operators.
So stellt die Wartezeit die einzige Möglichkeit dar um die Latenz des Pfades zu anzupassen.

Um minimale Ressourcen zu verbrauchen, startet der Algorithmus bei dem minimalen Parallelisierungsgrad für alle Operatoren.
Schrittweise berechnet er dann, bei welchem Operator eine Erhöhung des Parallelisierungsgrades um eins die größte Verringerung der Latenz des Pfades zur Folge hat.
Außerdem bestimmt der Algorithmus den Operator, welcher den zweitgrößten Effekt auf die Latenz des Pfades hat.
Für den Operator mit dem größten Effekt wird dann durch die von Lohrmann et al. definierte Funktion \(P_\Delta\) ein neuer Parallelisierungsgrad bestimmt.
Diese berechnet den neuen Parallelisierungsgrad in Abhängigkeit zum Operator mit dem zweitgrößten Effekt.
So soll verhindert werden, dass der Parallelisierungsgrad eines Operators mehrfach hintereinander um eins erhöht wird.
Stattdessen wird der Parallelisierungsgrad des gewählten Operators von \(P_\Delta\) so weit erhöht, dass in der nächsten Runde ein anderer Operator gewählt wird.
Diese Schritte werden so lange durchgeführt, bis der Pfad eine Latenz unter dem definierten Maximalwert aufweist.

\section{Implementation}

Im Folgenden sollen die Besonderheiten und Abweichungen vom Original in der vorliegenden Implementation des Algorithmus ausführlich diskutiert werden.
Die Implementation des Algorithmus von Lohrmann et al. verwendet das vorgestellte Graph-Modell.

\subsection{Latenz eines Tupels}
Eine weitere Feststellung ist für die Berechnung der Latenz eines einzelnen Tupels notwendig.
Laut Lohrmann et al. wird diese von dem Zeitpunkt an dem das Tupel von der Quelle emittiert wird bis zu dem Zeitpunkt an dem das Tupel an dem Konsument aufgenommen wird berechnet. 
Dabei ist nicht eindeutig definiert, ob die Latenz des Konsument-Operators berücksichtigt wird. 
In der vorliegenden Implementation wird die Latenz des Konsumentes ebenfalls zur Gesamtlatenz des Tupels gezählt.

\subsection{Initialisierung}
Für die fehlerfreie Berechnung des Parallelisierungsgrades ist es notwendig, dass das Modell so initialisiert ist, dass die zwischengespeicherten Messwerte größer als 0 sind.
Ist dies nicht der Fall kann es zu Divisionen durch 0 führen.
Daher wird vor der eigentlichen Ausführung des Algorithmus das Modell auf die korrekte Initialisierung geprüft. 
Falls dies nicht zutrifft, wird eine IllegalStateException geworfen, welche angibt, dass das Modell nicht ordnungsgemäß initialisiert ist.

\subsection{Minimaler Parallelisierungsgrad und Flaschenhals}
Die Wartezeit von Operator \(i\) wird von Lohrmann et. al mit der folgenden Funktion berechnet: \cite{lohrmann_elastic_2015}:
\[W(p_i^\ast) = e \left( \frac{\lambda_i {S}^{2}_{i} p_i}{p_i^\ast-\lambda_i {S}_i p_i}\right)\left(\frac{{c^{2}_{Ai}} + {c^{2}_{Si}}}{2}\right)\]
Der Wert des Koeffizienten \(e\) resultiert aus einer Angleichung der Ergebnisse des Modells an die Messwerte des realen CEP-Systems.
Er wird im nächsten Kapitel gesondert diskutiert.
\(\lambda\) ist die durchschnittliche Tupel-Ankunftsrate pro Millisekunde welche mit \[\frac{1}{Tupel-Ankunftsintervall}\] berechnet wird. 
\({S}\) beschreibt die Bearbeitungsdauer in Millisekunden.
\(p\) ist der aktuelle Parallelisierungsgrad des Operators.
Die aktuellen Messwerte aus dem CEP-System sind für diesen Parallelisierungsgrad gültig.
\(p^\ast\) ist der potentielle neue Parallelisierungsgrad des Operators.
Der Algorithmus versucht die Wartezeit durch das optimieren von \(p^\ast\) so anzupassen, dass die Latenzbeschränkung des Pfades eingehalten wird. Dabei aber möglichst wenig Ressourcen verwendet werden.
Der letzte Teil der Formel berechnet einen Koeffizienten aus den Varianzen der Bearbeitungsdauer und der Tupel-Ankunftsrate.
Essentiell für die folgenden Ausführungen ist vor allem der Nenner \(p_i^\ast-\lambda_i {S}_i p_i\).

Ein wichtiges Detail ist, dass ein negativer Nenner für die Funktion nicht sinnvoll ist.
Da die anderen beiden Koeffizienten immer positiv sind würde ein negativer Nenner im Ergebnis zu einer negativen Wartezeit führen.
Eine negative Wartezeit ist aber real nicht möglich, deswegen ist die Formel für diesen Anwendungsfall ungültig, wenn sie einen negativen Nenner besitzt.
Des Weiteren ist die Funktion offensichtlich ungültig wenn nach der Subtraktion im Nenner eine Null steht.

Um diese Probleme zu verhindern legen Lohrmann et al. fest, dass die Berechnungen des Algorithmus nur gültig sind, wenn in der Topologie keine Flaschenhälse vorhanden sind.
Ein Flaschenhals wird als ein Operator mit einer Auslastung von 100\% oder nahe 100\% definiert.
Um einen Flaschenhals aufzulösen wird im vorgeschlagenen Algorithmus der Parallelisierungsgrad des Operators verdoppelt. Falls die Auslastung höher als eins ist, wird der Parallelisierungsgrad noch mit der momentanen Auslastung multipliziert. Da der Parallelisierungsgrad eine Ganzzahl sein muss, muss das Produkt zu einem Integer umgewandelt werden. In der vorliegenden Implementation werden die Kommastellen nach der Multiplikation abgeschnitten und nicht gerundet. Bei einer Verdoppelung des Parallelisierungsgrades übt die maximale Differenz von 1 keinen starken Einfluss aus und der Code ist leichter zu lesen. Lohrmann et al. treffen selbst keine Aussage zu dieser Problematik.

Jedoch ist die Methode, Flaschenhälse in der Topologie aufzulösen, nicht ausreichend um die zuvor beschriebenen Probleme zu verhindern.
Die Auflösung der Flaschenhälse sorgt nur dafür, dass die Bedingung \( 0 < \lambda S < 1 \) erfüllt ist.
Um einen Nenner > 0 zu erhalten muss aber die Bedingung \(p^\ast > \lambda S p\) erfüllt sein.
Nehmen wir den Grenzwert \(1\) für die Auslastung (\(\lambda S\) an.
Dies bedeutet immer wenn \(p \geq p^\ast\) zutrifft ist der Nenner null oder negativ.
Lässt man die Auslastung gegen den Grenzwert \(0\) gehen, so ist der Nenner unter der Bedingung \(p \gg p^\ast\) null oder negativ.
Wie im ersten Teil dieses Kapitels beschrieben, berechnet der Algorithmus zu Beginn die Wartezeit mit dem minimalen Parallelisierungsgrad.
Wie in Kapitel ******** beschrieben ist es gewöhnlich, dass dieser eins beträgt.
Wir nehmen an der Algorithmus startet mit dem potentiellen Parallelisierungsgrad \(p^\ast\ = 1\).
Der momentan im System aktive Parallelisierungsgrad \(p\) bleibt für einen gesamten Durchlauf des Algorithmus konstant.
Da \(p^\ast = 1\) ist es nicht unwahrscheinlich, dass \(p \gg p^\ast\) zutrifft und somit der Nenner \(\leq 0\) ist.
Dies ist für die Anwendung aber nicht zulässig und stellt deshalb ein Fehler im von Lohrmann et al. vorgestellten Algorithmus dar.
Die Berechnung des Algorithmus darf nicht in allen Fällen mit dem im Modell festegelegten minimalen Parallelisierungsgrad von eins beginnen.
Stattdessen ist die Auswahl des minimalen Parallelisierungsgrades eines Operators wie folgt zu definieren: \(max(p_min, \lambda S p + 1\).
Die Erhöhung um eins ist notwendig um zu Verhindern, dass der Nenner null wird.
Diese Änderung wurde in der vorliegenden Implementation des Algorithmus umgesetzt.

\subsection{Ausnahme bei Flaschenhals mit maximalem Parallelisierungsgrad}
Wie zuvor beschrieben wird beim Auftreten einen Flaschenhalses der Parallelisierungsgrad des Operators verdoppelt. 
Dabei kann es vorkommen, dass der maximale Parallelisierungsgrad eines Operators überschritten wird. 
Die Implementation des Algorithmus wirft in diesen Fall eine Ausnahme, welche besagt dass der Operator trotz maximalem Parallelisierungsgrad ein Flaschenhals ist.
Es wird keine weitere Anpassung unternommen.

\subsection{Ausnahme bei nicht ausreichendem Parallelisierungsgrad}
Nachdem die Flaschenhälse aufgelöst wurden, prüft der Algorithmus, ob es mit der momentanen Konfiguration möglich ist den Grenzwert für die Latenz des Pfades zu erreichen,
Dazu wird die Latenz für den Fall berechnet, dass alle Operatoren bis zum maximalen Parallelisierungsgrad skaliert sind. Ist dies nicht der Fall, werden in der originalen Version des Algorithmus ohne weitere Mitteilung alle Operatoren maximal skaliert. In der vorliegenden Implementation wurde dieses Verhalten geändert, sodass eine IllegalStateException geworfen wird. Diese sagt aus, dass der maximale Parallelisierungsgrad nicht ausreichend ist. Eine Anpassung des Parallelisierungsgrades findet demnach nicht statt.

\subsection{Verhindere Loop durch zu kleinen Parallelisierungsgrad}
Wenn der Algorithmus schrittweise die Parallelisierungsgrade der Operatoren optimiert, wird die Funktion \(P\_\delta\) aufgerufen.
Sie bestimmt den nächsten Parallelisierungsgrad des Operators.
Entgegen der Intention der Funktion, dass in der nächsten Runde des Algorithmus ein anderer Operator gewählt wird, liefert Sie teilweise den aktuellen Parallelisierungsgrad des Operators.
Der Parallelisierungsgrad des gewählten Operators ändert sich dementsprechend nicht.
Dies führt offensichtlich dazu, dass er in der nächsten Runde wieder gewählt wird, da alle Parameter der Warteschlangen-Funktion identisch geblieben sind.
Dieses Verhalten führt zu einer endlosen Schleife im Algorithmus.
Um das Problem zu beheben wird in der Implementation das Ergebnis der Funktion \(P_\Delta\) geprüft.
Sei \(p\) der momentane Parallelisierungsgrad des Operators.
Das Ergebnis der Funktion \(P_\Delta\) ist \(p_Delta\).
Dann bestimmt sich der zukünftige Parallelisierungsgrad des Operators aus dem Maximum \(max(p+1, p_Delta\).

\subsection{Parallelisierungsgrad des letzten Operators}
In einem Spezialfall des Algorithmus wird die von Lohrmann et al. definierte Funktion \(P_w\) verwendet. 
Sie bestimmt den Parallelisierungsgrad des letzten verbleibenden Operators, wenn alle anderen Operatoren bereits maximal skaliert sind.
Allerdings weist diese im von Lohrmann et al. definierten Algorithmus keine Beschränkung durch den maximalen Parallelisierungsgrad des Operators auf. 
Es treten somit Fälle auf, in denen die Funktion einen Parallelisierungsgrade über dem Maximum zurückliefert.
Theoretisch sollte der Maximalwert nicht überschritten werden, denn zu Beginn überprüft der Algorithmus ob der Grenzwert für die Latenz unter maximaler Skalierung erreicht werden kann.
Dass dieser Fall dennoch eintritt hängt mit der im nächsten Kapitel beschriebenen Problematik zusammen.
Um diesen Fehler zu vermeiden wird in dieser Implementation das Ergebnis der Funktion geprüft und auf den maximalen Parallelisierungsgrad gesetzt, falls es diesen übersteigt.

\subsection{Koeffizient e}

Im Folgenden wird der Einfluss des Koeffizienten e auf die Funktion \(P_w\) untersucht.
Die Funktion ist wie folgt definiert:
\[P_w(i, w) = \lceil \frac{a_i}{w} + \lambda_i S_i p_i \rceil \]
\[a_i = \lambda_i S^{2}_{i} p_i \left(\frac{{c^{2}_{Ai}} + {c^{2}_{Si}}}{2}\right)\]

Der Parameter \(i\) bestimmt dabei den Index des Operators. 
Der Parameter \(w\) ist definiert als die für den Operator maximal erlaubte Wartezeit, um den Grenzwert für die Gesamtlatenz für den Pfad nicht zu überschreiten.
Diese ist bekannt, da \(i\) der letzte Operator ist, der noch nicht maximal skaliert ist.
Außerdem folgt aus der initialen Prüfung, dass die maximale Latenz zumindest bei maximaler Parallelisierung erreicht werden kann.

Die Funktion \(P_w\) ist die nach \(p_i^\ast\) umgestellte Variante der Funktion \(W(p_i^\ast\), die die Wartezeit eines Operators abhängig vom Parallelisierungsgrad bestimmt. 
Die umgestellte Formel bestimmt nun den Parallelisierungsgrad eines Operators abhängig von der maximal erlaubten Warteschlangenzeit \(w\). 
Allerdings wurde bei der Umstellung der Formel der Koeffizient \(e\) nicht berücksichtigt oder aus Pw absichtlich entfernt. 

Koeffizient \(e\) beschreibt die prozentuale Abweichung der gemessenen Wartezeit zur berechneten Wartezeit aus der Kingman-Formel. 
Er wird in der Funktion \(W\) benutzt um die Abweichung von Modell und realem System auszugleichen.
\(e\) wird wie folgt berechnet:
\[ e_i = \frac{l_{ji} - obl_{ji}}{Kingman_i}\]

wobei \(l_{ji}\) die Latenz des Kanals zwischen Operator \(i\) und dessen Vorgänger \(j\) beschreibt.
Die Latenz der Stapelverarbeitung am Ausgang von \(j\) wird durch \(obl_{ji}\) repräsentiert.
Ist die Netzwerklatenz in \(l_{ji}\) nicht Berücksichtigt ist die Differenz der beiden Latenzen ist die effektive Wartezeit eines Tupels im realen System.

Durch die fehlende Berücksichtigung von e kann der aus \(P_w\) resultierende Parallelisierungsgrad stark von dem in \(W\) angenommenen Wert abweichen. 
Angenommen die maximale Warteschlangendauer \(w = 1 \) wird in der Funktion \(W\) durch den Parallelisierungsgrad \(p^\ast = 1\) erfüllt. 
Es gilt also \(W(1)=1\).
Gleichzeitig nehmen wir an, dass \(e = 0,5\) beträgt. 
Der gemessene Wartezeit beträgt also nur 50\% des berechneten Wartezeit \(W(1) = 2 * 0,5\).
Der Koeffizient passt den berechneten Wert entsprechend an.
Berechnet man nun \(Pw\) für \(w = 1\) wird ein Parallelisierungsgrad von \(2\) zurückgeliefert, da dieser nicht um den Faktor \(e = 0,5\) angeglichen wurde. 
Der aus Pw resultierende Parallelisierungsgrad verschwendet also Ressourcen wenn angenommen wird, dass \(W(p)\) korrekt ist und der Parallelisierungsrad \(p=1\) genügt um \(w = 1\) zu erfüllen.

Nehmen wir nun an, dass \(p=1\) der maximale Parallelisierungsgrad eines Operators ist.
Um zu prüfen ob der Operator die maximal erlaubte Warteschlangenzeit \(w=1\) erfüllen kann, wird mit \(W(1) = 1\) geprüft ob er mit maximaler Auslastung diesen Wert erreicht . 
\(Pw\) bestimmt nun die tatsächliche Ausprägung des Parallelisierungsgrades und liefert den Wert \(2\), der den maximalen Parallelisierungsgrad überschreitet.

Drastischer ist das Problem für den Fall, dass der Faktor \(e > 1\) ist. 
Dann würde der gemessene Wert größer als der berechnete Wert sein. 
Nehmen wir an \(e = 1.5, W(3)=1\) und \(W(3) = (2/3) * 1.5\) sowie \(w=1\). 
Das Ergebnis von \(W\) ergibt, dass sich die maximale Warteschlangenzeit durch den Parallelisierungsgrad von 3 erfüllen lässt. 
Wird nun der Parallelisierungsgrad durch \(Pw\) berechnet resultiert daraus \(2\). 
Somit würde durch den geringeren Parallelisierungsgrad der Maximalwert für die Latenz des Pfades verletzt.

Um diesem Problem entgegen zu wirken, wird in der Implementation des Algorithmus das Ergebnis von \(P_w\) mit dem Koeffizienten \(e\) multipliziert.

\section{Parameter}

Dieser Bereich beschreibt die Parameter, mit welchen der Algorithmus gesteuert werden kann.
Parameter die in der originalen Version des Algorithmus von Lohrmann et al. fix definiert waren, wurden für die Implementation parametrisiert um flexibler zu sein.
Der Algorithmus berücksichtigt die neben den speziell dem Algorithmus zugewiesenen Parametern noch den maximalen und minimalen Parallelisierungsgrad aus dem Graph-Modell.
Die folgenden Parameter sind als finale statische Attribute in der Klasse für den Algorithmus zu finden.

\subsection{Flaschenhals Grenzwert}

Name: \textit{BOTTLENECK\_THRESHOLD}

Standardwert: \textit{1.0}

Wie von Lohrmann et al. beschrieben ist der Algorithmus ungültig wenn die Topologie einen Flaschenhals aufweist.
Ein Operator wird als Flaschenhals definiert, wenn er eine Auslastung von 100\% oder nahezu 100\% hat.
Für die Implementation wurde der Grenzwert von 100\% nicht als Konstante sondern als konfigurierbarer Parameter umgesetzt.
Der Wert kann als Dezimalzahl angegeben werden, sodass 1.0 = 100\% Auslastung entspricht.
Ein Wert von größer als 1.0 zu setzen ist für den Algorithmus nicht zulässig.
Allerdings könnte es für eine schnellere Skalierung der Operatoren interessant sein, dass der Grenzwert für einen Flaschenhals auf einen niedrigeren Wert gesetzt wird.

Allerdings ist zu beachten, dass der Wert nicht zu niedrig angesetzt sein darf, da sonst immer abwechselnd gegensätzliche Aktionen durch den Algorithmus angestoßen werden.
Im ersten Durchlauf erkennt er einen Flaschenhals, zum Beispiel mit einem Grenzwert von 50\%.
Daraufhin wir der Parallelisierungsgrad des Operators verdoppelt.
Beim nächsten Lauf ist der Flaschenhals nicht mehr vorhanden und der Algorithmus verwendet die Funktion der Wartezeit.
Er versucht die maximale Latenz des Pfades mit möglichst wenig Ressourcen zu unterschreiten.
Deshalb ist es sehr wahrscheinlich, dass er den zuvor verdoppelten Parallelisierungsgrad wieder zurück setzt um Ressourcen zu sparen.
Im darauf Folgenden lauf wird dies dazu führen, dass der Operator wieder als Flaschenhals erkannt wird.

\subsection{Koeffizient für Stapelverarbeitung}

Name: \textit{ADAPTIVE\_BATCHING\_COEFFICIENT}

Standardwert: \textit{0.8}

Wertebereich \textit{\(0 \leq x < 1\)}

Die Idee der Stapelverarbeitung ist, dass Tupel am Ende eines Operators gesammelt werden und gleichzeitig über das Netzwerk gesendet werden können.
Durch die Zusammenfassung der Tupel kann der Overhead für Netzwerkprotokolle verringert und somit der Durchsatz an Tupel verringert werden.
Allerdings steigt durch die Wartezeit im Stapel die Latenz.
Lohrmann et al. beschreiben in \cite{lohrmann_nephele_2014} adaptive Stapelverarbeitung.
Sie beschreiben ein System welches die Stapelgröße am Ende eines Operator dynamisch ändert, anstatt wie bei anderen CEP-Systemen, die Größe system-weit zu fixieren.
Der Name des Parameters ist darauf zurück zu führen, dass der vorgestellte Algorithmus auf dem CEP-System Nephele aufbaut, für das die adaptive Stapelverarbeitung implementiert wurde.
Jedoch ist die Aufgabe des Parameters im Algorithmus generell für jede Stapelverarbeitung zutreffend.

Der Parameter ist ein Koeffizient welcher die Dauer beschreibt, die ein Tupel in dem Ausgangsstapel eines Operators liegt.
Er wird als Anteil der maximalen Latenz des Pfades angegeben.
Die effektive maximale Latenz eines Pfades berechnet der Algorithmus durch das Produkt aus diesem Parameter und der maximalen Latenz des Pfades.

\subsection{Schrittweite}

Name: \textit{DELTA\_STEP\_SIZE}

Standardwert: \textit{1}

Wertebereich \textit{\(1 \leq x\)}

Dieser Parameter legt die Schrittweite fest, mit der der Algorithmus den Operator bestimmt, der den größten Einfluss auf die Latenz des Pfades hat.
In jeder Runde berechnet der Algorithmus die Wartezeit aller Operatoren.
Die Berechnung wird mit dem Parallelisierungsgrad , der um die definierte Schrittweite erhöht ist, durchgeführt. 
Anschließend vergleicht er, welcher Operator die Gesamtlatenz mit dem neuen Parallelisierungsgrad am stärksten verringern würde.
Die Schrittweite dient aber ausschließlich zur Auswahl des Operators.
Der Parallelisierungsgrad des Operators wird anschließend von der Funktion \(P_\Delta\) festgelegt.
Deswegen ist der Standardwert von eins durchaus sinnvoll.
Ein höherer Wert wäre interessanter, wenn die Operatoren um die Schrittweite ebenfalls direkt erhöht werden würden.
Für die vorliegende Version des Algorithmus führt die Abweichung vom Standardwert sehr wahrscheinlich zu einem höheren Verbrauch an Ressourcen.

\subsection{Verwendung der Latenz der Kanäle}

Name: \textit{USE\_LATENCY\_ADAPTION}

Standardwert: \textit{true}

Wertebereich \textit{Boolean}

Im einem vorhergehenden Kapitel wurde die Berechnung des Koeffizienten e untersucht.
Mit dem Koeffizienten wird versucht die berechnete Wartezeit an die Tatsächliche Wartezeit im System anzupassen.
Dazu wird die mit dem aktuellen Parallelisierungsgrad errechnete Wartezeit eines Operators mit der tatsächlich gemessenen Wartezeit ins Verhältnis gesetzt.
Das Messen der Wartezeit im realen System ist je nach Support des CEP-Systems nicht trivial.
Lohrmann et al. definieren die gemessene Wartezeit als \(Latenz des Kanals - Latenz der Stapelverarbeitung\).
Die Messung der Latenz des Kanals ist jedoch ein nicht triviales Problem, sobald Operatoren sich über mehrere Rechner verteilen.
Die Uhren der Hosts exakt synchron zu halten ist nicht realisieren.
Somit ist die Messung der Latenz des Kanals nie korrekt.

Ein weiteres Detail ist das Verhältnis der Netzwerklatenz zu der Wartezeit.
Wie zu Beginn dieses Kapitels beschrieben, ist die Wartezeit die variable Größe, die durch den Algorithmus optimiert wird.
Die Netzwerklatenz wird, wie die Latenz des Operators, als konstant angenommen.
Sind Operatoren über mehrere Rechner verteilt ist die Netzwerklatenz in der Latenz des Kanals enthalten.
Aus empirischer Erfahrung in der Testumgebung ist die Netzwerklatenz ein essentieller Teil der Latenz des Kanals.
Nehmen wir an dass die Netzwerklatenz einen Anteil von 90\% an der Gesamtlatenz von 10 ms hat.
Somit wäre die gemessene Wartezeit bei 1 ms.
Das Ergebnis für die Berechnung der Wartezeit durch die Kingman Formel liefert ebenfalls 1 ms.
Sie trifft also die reale Wartezeit des Tupels exakt.
Wenn die Latenz der Stapelverarbeitung mit null beziffern wird der Koeffizient e wird nun wie folgt berechnet:
\[ e = frac{Latenz des Kanals}{Kingman} = frac{10 / 1} = 10\]
Dies würde bedeuten dass der Algorithmus die Wartezeit des Operators verzehnfacht.
Anschließend würde er versuchen die verzehnfachte Wartezeit über den Parallelisierungsgrad des Operators zu verringern.
Real ist der Anteil der Netzwerk-Latenz aber konstant.
Im realen System werden durch die Skalierung somit nur 10\% der berechneten Verringerung der Wartezeit effektiv erreicht.
Der Algorithmus neigt deshalb dazu sehr hohe Parallelisierungsgrade zurück zu geben, die auf im realen aber nur einen geringen Effekt auf die Latenz des Pfades bewirken.

Daher ist eine Adaption der berechneten Wartezeit durch den Koeffizienten e nur sinnvoll, wenn die effektive Wartezeit im realen System bestimmt werden kann.
In der Implementation wurde daher der hier beschriebene Parameter eingeführt, um die Adaption durch den Koeffizienten e abzuschalten zu können.
Dieser wird sinnvoller Weise auf \textit{false} gesetzt wenn die Wartezeit im CEP-System nicht bestimmt werden kann.

















