% Encoding: UTF-8

%%%
%
%Beim Erstellen der Bibtex-Datei wird empfohlen darauf zu achten, dass die DOI aufgeführt wird.
%
%%%

@article{stonebraker_8_2005,
	title = {The 8 {Requirements} of {Real}-time {Stream} {Processing}},
	volume = {34},
	issn = {0163-5808},
	url = {http://doi.acm.org/10.1145/1107499.1107504},
	doi = {10.1145/1107499.1107504},
	abstract = {Applications that require real-time processing of high-volume data steams are pushing the limits of traditional data processing infrastructures. These stream-based applications include market feed processing and electronic trading on Wall Street, network and infrastructure monitoring, fraud detection, and command and control in military environments. Furthermore, as the "sea change" caused by cheap micro-sensor technology takes hold, we expect to see everything of material significance on the planet get "sensor-tagged" and report its state or location in real time. This sensorization of the real world will lead to a "green field" of novel monitoring and control applications with high-volume and low-latency processing requirements.Recently, several technologies have emerged---including off-the-shelf stream processing engines---specifically to address the challenges of processing high-volume, real-time data without requiring the use of custom code. At the same time, some existing software technologies, such as main memory DBMSs and rule engines, are also being "repurposed" by marketing departments to address these applications.In this paper, we outline eight requirements that a system software should meet to excel at a variety of real-time stream processing applications. Our goal is to provide high-level guidance to information technologists so that they will know what to look for when evaluation alternative stream processing solutions. As such, this paper serves a purpose comparable to the requirements papers in relational DBMSs and on-line analytical processing. We also briefly review alternative system software technologies in the context of our requirements.The paper attempts to be vendor neutral, so no specific commercial products are mentioned.},
	number = {4},
	urldate = {2018-04-27},
	journal = {SIGMOD Rec.},
	author = {Stonebraker, Michael and Çetintemel, Uǧur and Zdonik, Stan},
	month = dec,
	year = {2005},
	keywords = {Basic, Theorie},
	pages = {42--47},
	file = {ACM Full Text PDF:C\:\\Users\\Beni\\Zotero\\storage\\N8QSQ8RR\\Stonebraker et al. - 2005 - The 8 Requirements of Real-time Stream Processing.pdf:application/pdf}
}

@article{herbst_elasticity_nodate,
	title = {Elasticity in {Cloud} {Computing}: {What} {It} {Is}, and {What} {It} {Is} {Not}},
	abstract = {Originating from the ﬁeld of physics and economics, the term elasticity is nowadays heavily used in the context of cloud computing. In this context, elasticity is commonly understood as the ability of a system to automatically provision and deprovision computing resources on demand as workloads change. However, elasticity still lacks a precise deﬁnition as well as representative metrics coupled with a benchmarking methodology to enable comparability of systems. Existing deﬁnitions of elasticity are largely inconsistent and unspeciﬁc, which leads to confusion in the use of the term and its differentiation from related terms such as scalability and efﬁciency; the proposed measurement methodologies do not provide means to quantify elasticity without mixing it with efﬁciency or scalability aspects. In this short paper, we propose a precise deﬁnition of elasticity and analyze its core properties and requirements explicitly distinguishing from related terms such as scalability and efﬁciency. Furthermore, we present a set of appropriate elasticity metrics and sketch a new elasticity tailored benchmarking methodology addressing the special requirements on workload design and calibration.},
	language = {en},
	author = {Herbst, Nikolas Roman and Kounev, Samuel and Reussner, Ralf},
	pages = {6},
	file = {Herbst et al. - Elasticity in Cloud Computing What It Is, and Wha.pdf:C\:\\Users\\Beni\\Zotero\\storage\\IFAHS5K8\\Herbst et al. - Elasticity in Cloud Computing What It Is, and Wha.pdf:application/pdf}
}

@article{hidalgo_self-adaptive_2017,
	title = {Self-adaptive processing graph with operator fission for elastic stream processing},
	volume = {127},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121216300796},
	doi = {10.1016/j.jss.2016.06.010},
	abstract = {Nowadays, information generated by the Internet interactions is growing exponentially, creating massive and continuous flows of events from the most diverse sources. These interactions contain valuable information for domains such as government, commerce, and banks, among others. Extracting information in near real-time from such data requires powerful processing tools to cope with the high-velocity and the high-volume stream of events. Specially designed distributed processing engines build a graph-based topology of a static number of processing operators creating bottlenecks and load balance problems when processing dynamic flows of events. In this work we propose a self-adaptive processing graph that provides elasticity and scalability by automatically increasing or decreasing the number of processing operators to improve performance and resource utilization of the system. Our solution uses a model that monitors, analyzes and changes the graph topology with a control algorithm that is both reactive and proactive to the flow of events. We have evaluated our solution with three stream processing applications and results show that our model can adapt the graph topology when receiving events at high rate with sudden peaks, producing very low costs of memory and CPU usage.},
	urldate = {2018-06-06},
	journal = {Journal of Systems and Software},
	author = {Hidalgo, Nicolas and Wladdimiro, Daniel and Rosas, Erika},
	month = may,
	year = {2017},
	keywords = {S4, Henriette, nicht so gut, Zeitreihen Vorhersage},
	pages = {205--216},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Beni\\Zotero\\storage\\CTBVXCZN\\Hidalgo et al. - 2017 - Self-adaptive processing graph with operator fissi.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Beni\\Zotero\\storage\\YA74G6HU\\S0164121216300796.html:text/html;Summary - Self adapting processing Graph.pdf:C\:\\Users\\Beni\\Zotero\\storage\\W8UXKFSA\\Summary - Self adapting processing Graph.pdf:application/pdf}
}

@article{mayer_predictable_2015,
	title = {Predictable {Low}-{Latency} {Event} {Detection} {With} {Parallel} {Complex} {Event} {Processing}},
	volume = {2},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2015.2397316},
	abstract = {The tremendous number of sensors and smart objects being deployed in the Internet of Things (IoT) pose the potential for IT systems to detect and react to live-situations. For using this hidden potential, complex event processing (CEP) systems offer means to efficiently detect event patterns (complex events) in the sensor streams and therefore, help in realizing a “distributed intelligence” in the IoT. With the increasing number of data sources and the increasing volume at which data is produced, parallelization of event detection is crucial to limit the time events need to be buffered before they actually can be processed. In this paper, we propose a pattern-sensitive partitioning model for data streams that is capable of achieving a high degree of parallelism in detecting event patterns, which formerly could only consistently be detected in a sequential manner or at a low parallelization degree. Moreover, we propose methods to dynamically adapt the parallelization degree to limit the buffering imposed on event detection in the presence of dynamic changes to the workload. Extensive evaluations of the system behavior show that the proposed partitioning model allows for a high degree of parallelism and that the proposed adaptation methods are able to meet a buffering limit for event detection under high and dynamic workloads.},
	number = {4},
	journal = {IEEE Internet of Things Journal},
	author = {Mayer, R. and Koldehofe, B. and Rothermel, K.},
	month = aug,
	year = {2015},
	keywords = {parallel processing, Internet of Things, quality of service, Adaptation models, buffering limit, Complex event processing (CEP), data parallelization, data streams, Delays, dynamic workloads, Event detection, event pattern detection, Intelligent sensors, IoT, IT systems, Monitoring, parallel complex event processing, pattern-sensitive partitioning model, predictable low-latency event detection, self-adaptation, stream processing},
	pages = {274--286},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Beni\\Zotero\\storage\\H2RRN52Y\\7024105.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Beni\\Zotero\\storage\\FDJJJ7HJ\\Mayer et al. - 2015 - Predictable Low-Latency Event Detection With Paral.pdf:application/pdf}
}

@inproceedings{heinze_online_2015,
	address = {New York, NY, USA},
	series = {{SoCC} '15},
	title = {Online {Parameter} {Optimization} for {Elastic} {Data} {Stream} {Processing}},
	isbn = {978-1-4503-3651-2},
	url = {http://doi.acm.org/10.1145/2806777.2806847},
	doi = {10.1145/2806777.2806847},
	abstract = {Elastic scaling allows data stream processing systems to dynamically scale in and out to react to workload changes. As a consequence, unexpected load peaks can be handled and the extent of the overprovisioning can be reduced. However, the strategies used for elastic scaling of such systems need to be tuned manually by the user. This is an error prone and cumbersome task, because it requires a detailed knowledge of the underlying system and workload characteristics. In addition, the resulting quality of service for a specific scaling strategy is unknown a priori and can be measured only during runtime. In this paper we present an elastic scaling data stream processing prototype, which allows to trade off monetary cost against the offered quality of service. To that end, we use an online parameter optimization, which minimizes the monetary cost for the user. Using our prototype a user is able to specify the expected quality of service as an input to the optimization, which automatically detects significant changes of the workload pattern and adjusts the elastic scaling strategy based on the current workload characteristics. Our prototype is able to reduce the costs for three real-world use cases by 19\% compared to a naive parameter setting and by 10\% compared to a manually tuned system. In contrast to state of the art solutions, our system provides a stable and good trade-off between monetary cost and quality of service.},
	urldate = {2018-06-06},
	booktitle = {Proceedings of the {Sixth} {ACM} {Symposium} on {Cloud} {Computing}},
	publisher = {ACM},
	author = {Heinze, Thomas and Roediger, Lars and Meister, Andreas and Ji, Yuanzhen and Jerzak, Zbigniew and Fetzer, Christof},
	year = {2015},
	keywords = {elasticity, distributed data stream processing, load balancing, parameter optimization},
	pages = {276--287},
	file = {ACM Full Text PDF:C\:\\Users\\Beni\\Zotero\\storage\\QPPFUPKV\\Heinze et al. - 2015 - Online Parameter Optimization for Elastic Data Str.pdf:application/pdf}
}

@article{liu_performance-oriented_2017,
	title = {Performance-{Oriented} {Deployment} of {Streaming} {Applications} on {Cloud}},
	issn = {2332-7790},
	url = {http://ieeexplore.ieee.org/document/7962193/},
	doi = {10.1109/TBDATA.2017.2720622},
	abstract = {Performance of streaming applications are signiﬁcantly impacted by the deployment decisions made at infrastructure level, i.e., number and conﬁguration of resources allocated for each functional unit of the application. The current deployment practices are mostly platform-oriented, meaning that the deployment conﬁguration is tuned to a static resource-set environment and thus is inﬂexible to use in cloud with an on-demand resource pool. In this paper, we propose P-Deployer, a deployment framework that enables streaming applications to run on IaaS clouds with satisfactory performance and minimal resource consumption. It achieves performance-oriented, cost-efﬁcient and automated deployment by holistically optimizing the decisions of operator parallelization, resource provisioning, and task mapping. Using a Monitor-Analyze-Plan-Execute (MAPE) architecture, P-Deployer iteratively builds the connection between performance outcome and resource consumption through task proﬁling and models the deployment problem as a bin-packing variant. Extensive experiments using both synthetic and real-world streaming applications have shown the correctness and scalability of our approach, and demonstrated its superiority compared to platform-oriented methods in terms of resource cost.},
	language = {en},
	urldate = {2018-04-18},
	journal = {IEEE Transactions on Big Data},
	author = {Liu, Xunyun and Buyya, Rajkumar},
	year = {2017},
	pages = {1--1},
	file = {Liu und Buyya - 2017 - Performance-Oriented Deployment of Streaming Appli.pdf:C\:\\Users\\Beni\\Zotero\\storage\\45WFURUK\\Liu und Buyya - 2017 - Performance-Oriented Deployment of Streaming Appli.pdf:application/pdf}
}

@inproceedings{heinze_auto-scaling_2014,
	title = {Auto-scaling techniques for elastic data stream processing},
	doi = {10.1109/ICDEW.2014.6818344},
	abstract = {An elastic data stream processing system is able to handle changes in workload by dynamically scaling out and scaling in. This allows for handling of unexpected load spikes without the need for constant overprovisioning. One of the major challenges for an elastic system is to find the right point in time to scale in or to scale out. Finding such a point is difficult as it depends on constantly changing workload and system characteristics. In this paper we investigate the application of different auto-scaling techniques for solving this problem. Specifically: (1) we formulate basic requirements for an auto-scaling technique used in an elastic data stream processing system (2) we use the formulated requirements to select the best auto scaling techniques and (3) we perform evaluation of the selected auto scaling techniques using the real world data. Our experiments show that the auto scaling techniques used in existing elastic data stream processing systems are performing worse than the strategies used in our work.},
	booktitle = {2014 {IEEE} 30th {International} {Conference} on {Data} {Engineering} {Workshops}},
	author = {Heinze, T. and Pappalardo, V. and Jerzak, Z. and Fetzer, C.},
	month = mar,
	year = {2014},
	pages = {296--302},
	file = {Heinze et al. - 2014 - Auto-scaling techniques for elastic data stream pr.pdf:C\:\\Users\\Beni\\Zotero\\storage\\95R5Y34M\\Heinze et al. - 2014 - Auto-scaling techniques for elastic data stream pr.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Beni\\Zotero\\storage\\CFR7XALK\\6818344.html:text/html}
}

@article{gedik_elastic_2014,
	title = {Elastic {Scaling} for {Data} {Stream} {Processing}},
	volume = {25},
	issn = {1045-9219},
	url = {http://ieeexplore.ieee.org/document/6678504/},
	doi = {10.1109/TPDS.2013.295},
	abstract = {This article addresses the profitability problem associated with auto-parallelization of general-purpose distributed data stream processing applications. Auto-parallelization involves locating regions in the application’s data flow graph that can be replicated at run-time to apply data partitioning, in order to achieve scale. In order to make auto-parallelization effective in practice, the profitability question needs to be answered: How many parallel channels provide the best throughput? The answer to this question changes depending on the workload dynamics and resource availability at run-time. In this article, we propose an elastic auto-parallelization solution that can dynamically adjust the number of channels used to achieve high throughput without unnecessarily wasting resources. Most importantly, our solution can handle partitioned stateful operators via run-time state migration, which is fully transparent to the application developers. We provide an implementation and evaluation of the system on an industrial-strength data stream processing platform to validate our solution.},
	language = {en},
	number = {6},
	urldate = {2018-04-27},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Gedik, Bugra and Schneider, Scott and Hirzel, Martin and Wu, Kun-Lung},
	month = jun,
	year = {2014},
	keywords = {Basic, tbr, Theorie},
	pages = {1447--1463},
	file = {Gedik et al. - 2014 - Elastic Scaling for Data Stream Processing.pdf:C\:\\Users\\Beni\\Zotero\\storage\\AIHQVEG5\\Gedik et al. - 2014 - Elastic Scaling for Data Stream Processing.pdf:application/pdf}
}

@inproceedings{lohrmann_elastic_2015,
	title = {Elastic {Stream} {Processing} with {Latency} {Guarantees}},
	isbn = {978-1-4673-7214-5},
	url = {http://ieeexplore.ieee.org/document/7164926/},
	doi = {10.1109/ICDCS.2015.48},
	abstract = {Many Big Data applications in science and industry have arisen, that require large amounts of streamed or event data to be analyzed with low latency. This paper presents a reactive strategy to enforce latency guarantees in data ﬂows running on scalable Stream Processing Engines (SPEs), while minimizing resource consumption. We introduce a model for estimating the latency of a data ﬂow, when the degrees of parallelism of the tasks within are changed. We describe how to continuously measure the necessary performance metrics for the model, and how it can be used to enforce latency guarantees, by determining appropriate scaling actions at runtime. Therefore, it leverages the elasticity inherent to common cloud technology and cluster resource management systems. We have implemented our strategy as part of the Nephele SPE. To showcase the effectiveness of our approach, we provide an experimental evaluation on a large commodity cluster, using both a synthetic workload as well as an application performing real-time sentiment analysis on real-world social media data.},
	language = {en},
	urldate = {2018-04-18},
	publisher = {IEEE},
	author = {Lohrmann, Bjorn and Janacik, Peter and Kao, Odej},
	month = jun,
	year = {2015},
	pages = {399--410},
	file = {Lohrmann et al. - 2015 - Elastic Stream Processing with Latency Guarantees.pdf:C\:\\Users\\Beni\\Zotero\\storage\\85S2Q37E\\Lohrmann et al. - 2015 - Elastic Stream Processing with Latency Guarantees.pdf:application/pdf}
}

@inproceedings{kombi_preventive_2017,
	title = {A {Preventive} {Auto}-{Parallelization} {Approach} for {Elastic} {Stream} {Processing}},
	doi = {10.1109/ICDCS.2017.253},
	abstract = {Nowadays, more and more sources (connected devices, social networks, etc.) emit real-time data with fluctuating rates over time. Existing distributed stream processing engines (SPE) have to resolve a difficult problem: deliver results satisfying end-users in terms of quality and latency without over-consuming resources. This paper focuses on parallelization of operators to adapt their throughput to their input rate. We suggest an approach which prevents operator congestion in order to limit degradation of results quality. This approach relies on an automatic and dynamic adaptation of resource consumption for each continuous query. This solution takes advantage of i) a metric estimating the activity level of operators in the near future ii) the AUTOSCALE approach which evaluates the need to modify parallelism degrees at local and global scope iii) an integration into the Apache Storm solution. We show performance tests comparing our approach to the native solution of this SPE.},
	booktitle = {2017 {IEEE} 37th {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	author = {Kombi, R. K. and Lumineau, N. and Lamarre, P.},
	month = jun,
	year = {2017},
	keywords = {Storm, Survey Henriette, gut, Nachfolger Operatoren berücksichtigt, Zeitreihen Vorhersage},
	pages = {1532--1542},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Beni\\Zotero\\storage\\K4QEH4ZW\\7980091.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Beni\\Zotero\\storage\\NIVNJND4\\Kombi et al. - 2017 - A Preventive Auto-Parallelization Approach for Ela.pdf:application/pdf;Summary - A Preventive Auto Parallelization.pdf:C\:\\Users\\Beni\\Zotero\\storage\\HLYZFXET\\Summary - A Preventive Auto Parallelization.pdf:application/pdf}
}

@article{lohrmann_nephele_2014,
	title = {Nephele streaming: stream processing under {QoS} constraints at scale},
	volume = {17},
	issn = {1386-7857, 1573-7543},
	shorttitle = {Nephele streaming},
	url = {http://link.springer.com/10.1007/s10586-013-0281-8},
	doi = {10.1007/s10586-013-0281-8},
	abstract = {The ability to process large numbers of continuous data streams in a near-real-time fashion has become a crucial prerequisite for many scientiﬁc and industrial use cases in recent years. While the individual data streams are usually trivial to process, their aggregated data volumes easily exceed the scalability of traditional stream processing systems.},
	language = {en},
	number = {1},
	urldate = {2018-04-23},
	journal = {Cluster Computing},
	author = {Lohrmann, Björn and Warneke, Daniel and Kao, Odej},
	month = mar,
	year = {2014},
	keywords = {tbr},
	pages = {61--78},
	file = {Lohrmann et al. - 2014 - Nephele streaming stream processing under QoS con.pdf:C\:\\Users\\Beni\\Zotero\\storage\\58J3DKCU\\Lohrmann et al. - 2014 - Nephele streaming stream processing under QoS con.pdf:application/pdf}
}

@misc{noauthor_heron_nodate,
	title = {Heron {Metrics}},
	url = {https://apache.github.io/incubator-heron/docs/operators/heron-tracker-api/#topologies_metrics}
}

@inproceedings{zacheilas_elastic_2015,
	title = {Elastic complex event processing exploiting prediction},
	isbn = {978-1-4799-9926-2},
	url = {http://ieeexplore.ieee.org/document/7363758/},
	doi = {10.1109/BigData.2015.7363758},
	urldate = {2018-06-06},
	publisher = {IEEE},
	author = {Zacheilas, Nikos and Kalogeraki, Vana and Zygouras, Nikolas and Panagiotou, Nikolaos and Gunopulos, Dimitrios},
	month = oct,
	year = {2015},
	pages = {213--222},
	file = {Zacheilas et al. - 2015 - Elastic complex event processing exploiting predic.pdf:C\:\\Users\\Beni\\Zotero\\storage\\M5Q6J3T8\\Zacheilas et al. - 2015 - Elastic complex event processing exploiting predic.pdf:application/pdf}
}

@article{de_assuncao_distributed_2017,
	title = {Distributed {Data} {Stream} {Processing} and {Edge} {Computing}: {A} {Survey} on {Resource} {Elasticity} and {Future} {Directions}},
	shorttitle = {Distributed {Data} {Stream} {Processing} and {Edge} {Computing}},
	url = {http://arxiv.org/abs/1709.01363},
	abstract = {Under several emerging application scenarios, such as in smart cities, operational monitoring of large infrastructure, wearable assistance, and Internet of Things, continuous data streams must be processed under very short delays. Several solutions, including multiple software engines, have been developed for processing unbounded data streams in a scalable and eﬃcient manner. More recently, architecture has been proposed to use edge computing for data stream processing. This paper surveys state of the art on stream processing engines and mechanisms for exploiting resource elasticity features of cloud computing in stream processing. Resource elasticity allows for an application or service to scale out/in according to ﬂuctuating demands. Although such features have been extensively investigated for enterprise applications, stream processing poses challenges on achieving elastic systems that can make eﬃcient resource management decisions based on current load. Elasticity becomes even more challenging in highly distributed environments comprising edge and cloud computing resources. This work examines some of these challenges and discusses solutions proposed in the literature to address them.},
	language = {en},
	urldate = {2018-10-12},
	journal = {arXiv:1709.01363 [cs]},
	author = {de Assuncao, Marcos Dias and Veith, Alexandre da Silva and Buyya, Rajkumar},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.01363},
	keywords = {C.2.4, Computer Science - Distributed, Parallel, and Cluster Computing, H.3.4},
	file = {de Assuncao et al. - 2017 - Distributed Data Stream Processing and Edge Comput.pdf:C\:\\Users\\Beni\\Zotero\\storage\\E479NXSH\\de Assuncao et al. - 2017 - Distributed Data Stream Processing and Edge Comput.pdf:application/pdf}
}

@inproceedings{kulkarni_twitter_2015,
	address = {New York, NY, USA},
	series = {{SIGMOD} '15},
	title = {Twitter {Heron}: {Stream} {Processing} at {Scale}},
	isbn = {978-1-4503-2758-9},
	shorttitle = {Twitter {Heron}},
	url = {http://doi.acm.org/10.1145/2723372.2742788},
	doi = {10.1145/2723372.2742788},
	abstract = {Storm has long served as the main platform for real-time analytics at Twitter. However, as the scale of data being processed in real-time at Twitter has increased, along with an increase in the diversity and the number of use cases, many limitations of Storm have become apparent. We need a system that scales better, has better debug-ability, has better performance, and is easier to manage -- all while working in a shared cluster infrastructure. We considered various alternatives to meet these needs, and in the end concluded that we needed to build a new real-time stream data processing system. This paper presents the design and implementation of this new system, called Heron. Heron is now the de facto stream data processing engine inside Twitter, and in this paper we also share our experiences from running Heron in production. In this paper, we also provide empirical evidence demonstrating the efficiency and scalability of Heron.



This article is summarized in:
the morning paper

an interesting/influential/important paper from the world of CS every weekday morning, as selected by Adrian Colyer},
	urldate = {2018-10-08},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Kulkarni, Sanjeev and Bhagat, Nikunj and Fu, Maosong and Kedigehalli, Vikas and Kellogg, Christopher and Mittal, Sailesh and Patel, Jignesh M. and Ramasamy, Karthik and Taneja, Siddarth},
	year = {2015},
	keywords = {real-time data processing., stream data processing systems},
	pages = {239--250},
	file = {ACM Full Text PDF:C\:\\Users\\Beni\\Zotero\\storage\\L4PZP3G9\\Kulkarni et al. - 2015 - Twitter Heron Stream Processing at Scale.pdf:application/pdf}
}

@incollection{rasmussen2004gaussian,
  title={Gaussian processes in machine learning},
  author={Rasmussen, Carl Edward},
  booktitle={Advanced lectures on machine learning},
  pages={63--71},
  year={2004},
  publisher={Springer}
}

@inproceedings{schneider_elastic_2009,
	title = {Elastic scaling of data parallel operators in stream processing},
	doi = {10.1109/IPDPS.2009.5161036},
	abstract = {We describe an approach to elastically scale the performance of a data analytics operator that is part of a streaming application. Our techniques focus on dynamically adjusting the amount of computation an operator can carry out in response to changes in incoming workload and the availability of processing cycles. We show that our elastic approach is beneficial in light of the dynamic aspects of streaming workloads and stream processing environments. Addressing another recent trend, we show the importance of our approach as a means to providing computational elasticity in multicore processor-based environments such that operators can automatically find their best operating point. Finally, we present experiments driven by synthetic workloads, showing the space where the optimizing efforts are most beneficial and a radioastronomy imaging application, where we observe substantial improvements in its performance-critical section.},
	booktitle = {2009 {IEEE} {International} {Symposium} on {Parallel} {Distributed} {Processing}},
	author = {Schneider, S. and Andrade, H. and Gedik, B. and Biem, A. and Wu, K. L.},
	month = may,
	year = {2009},
	keywords = {elastic scaling, parallel processing, Intelligent sensors, Multicore processing, multiprocessing systems, Runtime, Streaming media, Elasticity, Application software, Availability, computational elasticity, Computer science, Data analysis, data analytics operator, data parallel operator, media streaming, multicore processor, Performance analysis, radioastronomy imaging, stream processing environment, workload streaming},
	pages = {1--12},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Beni\\Zotero\\storage\\D4L9K7AL\\5161036.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Beni\\Zotero\\storage\\I5XCFJYR\\Schneider et al. - 2009 - Elastic scaling of data parallel operators in stre.pdf:application/pdf}
}

@misc{noauthor_get_nodate,
	title = {{GET} statuses/sample — {Twitter} {Developers}},
	url = {https://developer.twitter.com/en/docs/tweets/sample-realtime/overview/GET_statuse_sample},
	urldate = {2018-10-09},
	file = {GET statuses/sample — Twitter Developers:C\:\\Users\\Beni\\Zotero\\storage\\S2BT2KTA\\GET_statuse_sample.html:text/html}
}

@misc{noauthor_welcome_nodate,
	title = {Welcome to {JGraphT} - a free {Java} {Graph} {Library}},
	url = {https://jgrapht.org/},
	urldate = {2018-10-08},
	file = {Welcome to JGraphT - a free Java Graph Library:C\:\\Users\\Beni\\Zotero\\storage\\9YUNI8UE\\jgrapht.org.html:text/html}
}

@misc{noauthor_smile_nodate,
	title = {Smile - {Statistical} {Machine} {Intelligence} and {Learning} {Engine}},
	url = {http://haifengl.github.io/smile/},
	urldate = {2018-10-08},
	file = {Smile - Statistical Machine Intelligence and Learning Engine:C\:\\Users\\Beni\\Zotero\\storage\\VJT7SYAE\\smile.html:text/html}
}


@misc{noauthor_how_2017,
	title = {How much is 1\%?},
	url = {https://twittercommunity.com/t/how-much-is-1/95878},
	abstract = {Hello,  I haven’t found any clear answer regarding the 1\% limit for the streaming API.  I understand that when extracting real-time tweets from the Streaming API, we are allowed to 1\% of all the tweets “at any given time”.  I dont understand “at any given time”.  For example I extracted 100 000 tweets within 1 hour duration (from 6 to 7 )using the Streaming API. Does this means that during 6 to 7pm, 10 000 000 tweets were tweeted to twitter and so i extracted only 1\%?  thank you},
	language = {en},
	urldate = {2018-10-09},
	journal = {Twitter Developers},
	month = nov,
	year = {2017},
	file = {Snapshot:C\:\\Users\\Beni\\Zotero\\storage\\GWJ5H9TC\\95878.html:text/html}
}

@Comment{jabref-meta: databaseType:biblatex;}
