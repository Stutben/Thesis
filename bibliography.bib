% Encoding: UTF-8

%%%
%
%Beim Erstellen der Bibtex-Datei wird empfohlen darauf zu achten, dass die DOI aufgeführt wird.
%
%%%

@inproceedings{lohrmann_elastic_2015,
	title = {Elastic {Stream} {Processing} with {Latency} {Guarantees}},
	isbn = {978-1-4673-7214-5},
	url = {http://ieeexplore.ieee.org/document/7164926/},
	doi = {10.1109/ICDCS.2015.48},
	abstract = {Many Big Data applications in science and industry have arisen, that require large amounts of streamed or event data to be analyzed with low latency. This paper presents a reactive strategy to enforce latency guarantees in data ﬂows running on scalable Stream Processing Engines (SPEs), while minimizing resource consumption. We introduce a model for estimating the latency of a data ﬂow, when the degrees of parallelism of the tasks within are changed. We describe how to continuously measure the necessary performance metrics for the model, and how it can be used to enforce latency guarantees, by determining appropriate scaling actions at runtime. Therefore, it leverages the elasticity inherent to common cloud technology and cluster resource management systems. We have implemented our strategy as part of the Nephele SPE. To showcase the effectiveness of our approach, we provide an experimental evaluation on a large commodity cluster, using both a synthetic workload as well as an application performing real-time sentiment analysis on real-world social media data.},
	language = {en},
	urldate = {2018-04-18},
	publisher = {IEEE},
	author = {Lohrmann, Bjorn and Janacik, Peter and Kao, Odej},
	month = jun,
	year = {2015},
	pages = {399--410},
	file = {Lohrmann et al. - 2015 - Elastic Stream Processing with Latency Guarantees.pdf:C\:\\Users\\Beni\\Zotero\\storage\\85S2Q37E\\Lohrmann et al. - 2015 - Elastic Stream Processing with Latency Guarantees.pdf:application/pdf}
}

@article{lohrmann_nephele_2014,
	title = {Nephele streaming: stream processing under {QoS} constraints at scale},
	volume = {17},
	issn = {1386-7857, 1573-7543},
	shorttitle = {Nephele streaming},
	url = {http://link.springer.com/10.1007/s10586-013-0281-8},
	doi = {10.1007/s10586-013-0281-8},
	abstract = {The ability to process large numbers of continuous data streams in a near-real-time fashion has become a crucial prerequisite for many scientiﬁc and industrial use cases in recent years. While the individual data streams are usually trivial to process, their aggregated data volumes easily exceed the scalability of traditional stream processing systems.},
	language = {en},
	number = {1},
	urldate = {2018-04-23},
	journal = {Cluster Computing},
	author = {Lohrmann, Björn and Warneke, Daniel and Kao, Odej},
	month = mar,
	year = {2014},
	keywords = {tbr},
	pages = {61--78},
	file = {Lohrmann et al. - 2014 - Nephele streaming stream processing under QoS con.pdf:C\:\\Users\\Beni\\Zotero\\storage\\58J3DKCU\\Lohrmann et al. - 2014 - Nephele streaming stream processing under QoS con.pdf:application/pdf}
}

@misc{noauthor_heron_nodate,
	title = {Heron {Metrics}},
	url = {https://apache.github.io/incubator-heron/docs/operators/heron-tracker-api/#topologies_metrics}
}

@inproceedings{zacheilas_elastic_2015,
	title = {Elastic complex event processing exploiting prediction},
	isbn = {978-1-4799-9926-2},
	url = {http://ieeexplore.ieee.org/document/7363758/},
	doi = {10.1109/BigData.2015.7363758},
	urldate = {2018-06-06},
	publisher = {IEEE},
	author = {Zacheilas, Nikos and Kalogeraki, Vana and Zygouras, Nikolas and Panagiotou, Nikolaos and Gunopulos, Dimitrios},
	month = oct,
	year = {2015},
	pages = {213--222},
	file = {Zacheilas et al. - 2015 - Elastic complex event processing exploiting predic.pdf:C\:\\Users\\Beni\\Zotero\\storage\\M5Q6J3T8\\Zacheilas et al. - 2015 - Elastic complex event processing exploiting predic.pdf:application/pdf}
}

@inproceedings{kulkarni_twitter_2015,
	address = {New York, NY, USA},
	series = {{SIGMOD} '15},
	title = {Twitter {Heron}: {Stream} {Processing} at {Scale}},
	isbn = {978-1-4503-2758-9},
	shorttitle = {Twitter {Heron}},
	url = {http://doi.acm.org/10.1145/2723372.2742788},
	doi = {10.1145/2723372.2742788},
	abstract = {Storm has long served as the main platform for real-time analytics at Twitter. However, as the scale of data being processed in real-time at Twitter has increased, along with an increase in the diversity and the number of use cases, many limitations of Storm have become apparent. We need a system that scales better, has better debug-ability, has better performance, and is easier to manage -- all while working in a shared cluster infrastructure. We considered various alternatives to meet these needs, and in the end concluded that we needed to build a new real-time stream data processing system. This paper presents the design and implementation of this new system, called Heron. Heron is now the de facto stream data processing engine inside Twitter, and in this paper we also share our experiences from running Heron in production. In this paper, we also provide empirical evidence demonstrating the efficiency and scalability of Heron.



This article is summarized in:
the morning paper

an interesting/influential/important paper from the world of CS every weekday morning, as selected by Adrian Colyer},
	urldate = {2018-10-08},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Kulkarni, Sanjeev and Bhagat, Nikunj and Fu, Maosong and Kedigehalli, Vikas and Kellogg, Christopher and Mittal, Sailesh and Patel, Jignesh M. and Ramasamy, Karthik and Taneja, Siddarth},
	year = {2015},
	keywords = {real-time data processing., stream data processing systems},
	pages = {239--250},
	file = {ACM Full Text PDF:C\:\\Users\\Beni\\Zotero\\storage\\L4PZP3G9\\Kulkarni et al. - 2015 - Twitter Heron Stream Processing at Scale.pdf:application/pdf}
}

@incollection{rasmussen2004gaussian,
  title={Gaussian processes in machine learning},
  author={Rasmussen, Carl Edward},
  booktitle={Advanced lectures on machine learning},
  pages={63--71},
  year={2004},
  publisher={Springer}
}

@misc{noauthor_get_nodate,
	title = {{GET} statuses/sample — {Twitter} {Developers}},
	url = {https://developer.twitter.com/en/docs/tweets/sample-realtime/overview/GET_statuse_sample},
	urldate = {2018-10-09},
	file = {GET statuses/sample — Twitter Developers:C\:\\Users\\Beni\\Zotero\\storage\\S2BT2KTA\\GET_statuse_sample.html:text/html}
}

@misc{noauthor_welcome_nodate,
	title = {Welcome to {JGraphT} - a free {Java} {Graph} {Library}},
	url = {https://jgrapht.org/},
	urldate = {2018-10-08},
	file = {Welcome to JGraphT - a free Java Graph Library:C\:\\Users\\Beni\\Zotero\\storage\\9YUNI8UE\\jgrapht.org.html:text/html}
}

@misc{noauthor_smile_nodate,
	title = {Smile - {Statistical} {Machine} {Intelligence} and {Learning} {Engine}},
	url = {http://haifengl.github.io/smile/},
	urldate = {2018-10-08},
	file = {Smile - Statistical Machine Intelligence and Learning Engine:C\:\\Users\\Beni\\Zotero\\storage\\VJT7SYAE\\smile.html:text/html}
}


@misc{noauthor_how_2017,
	title = {How much is 1\%?},
	url = {https://twittercommunity.com/t/how-much-is-1/95878},
	abstract = {Hello,  I haven’t found any clear answer regarding the 1\% limit for the streaming API.  I understand that when extracting real-time tweets from the Streaming API, we are allowed to 1\% of all the tweets “at any given time”.  I dont understand “at any given time”.  For example I extracted 100 000 tweets within 1 hour duration (from 6 to 7 )using the Streaming API. Does this means that during 6 to 7pm, 10 000 000 tweets were tweeted to twitter and so i extracted only 1\%?  thank you},
	language = {en},
	urldate = {2018-10-09},
	journal = {Twitter Developers},
	month = nov,
	year = {2017},
	file = {Snapshot:C\:\\Users\\Beni\\Zotero\\storage\\GWJ5H9TC\\95878.html:text/html}
}

@Comment{jabref-meta: databaseType:biblatex;}
